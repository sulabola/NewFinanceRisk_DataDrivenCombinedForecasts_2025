---
title: "Rcode_Price_COMPSAC_2025"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
rm(list=ls(all=TRUE)) # Remove objects from environment
set.seed(123)
```


```{r}
### Load Packages
library(quantmod) ## To download data from Yahoo finance
library(forecast) ## To fit ARIMA model
library(ggplot2)  ## For lag plots
library(gridExtra) ## For arranging multiple plots
library(grid)      ## For grid.draw function
library(tseries) ## For adf.test
library(xtable) ## For Latex Tables
library(writexl) ## Write Excel files

library(dplyr)

library(fpp3)            ## Loads all necessary packages, including fable, tsibble, etc.

```


Note: According to Yahoo Finance there are 11 sectors in the stock market in total, each with its own characteristics and features. Under each sector umbrella is a grouping of industries, which are represented by all the companies in that industry that trade on the stock market.

Technology - 12 industries

Financial Services - 15 industries

Consumer Cyclical - 23 industries

Healthcare - 11 industries

Industrials - 25 industries

Communication Services - 7 industries

Consumer Defensive - 12 industries

Energy - 8 industries

Real Estate - 12 industries

Basic Materials - 14 industries

Utilities - 6 industries


Here we are considering 10 stocks ('AAPL', 'BRK-B', 'AMZN', 'LLY', 'GE', 'WMT', 'TXGE', 'PLD', 'LIN', 'NEE') covering each sector for analysis (omit Sector6: Communication services). Stocks are choose with highest market capital.

Note: Code is setup for one stock. Change the ticker symbol to get resulsts for other stocks.


```{r}
start_date <- '2020-03-11'
end_date <- '2023-05-05'

# Stock
ticker <- 'AAPL'

# Download data for each ticker and extract adjusted closing prices
  price_data <- lapply(ticker, function(ticker) {
    data <- getSymbols(ticker, src = 'yahoo', from = start_date, to = end_date, auto.assign = FALSE)
    adj_close <- Ad(data)
    colnames(adj_close) <- ticker # Rename column to the ticker symbol
    return(adj_close)
  })
  
# Combine the data into a single data frame
stocks <- do.call(cbind, price_data)
  
# Convert to a data frame and remove rows with NA values
asset <- as.data.frame(stocks)
asset <- na.omit(asset)
  
# Ensure column names are correct
colnames(asset) <- colnames(stocks)

head(asset)

```


#### Fit ARIMA Models for stock prices


```{r}
# Initialize an empty data frame to store ARIMA model orders
arima_results <- data.frame(
  Stock = character(),
  ARIMA_Order = character(),
  stringsAsFactors = FALSE
)
  
# Fit ARIMA model to the column asset
arima_model <- auto.arima(asset)
    
# Collect ARIMA order (p, d, q)
arima_order <- paste("(", arima_model$arma[1], ",", arima_model$arma[6], ",", arima_model$arma[2], ")", sep = "")
    
# Append the results to the data frame
arima_results <- data.frame(
      Stock = colnames(asset),
      ARIMA_Order = paste("ARIMA", arima_order)
    )

print(arima_results)

```


Comments on ARIMA Model: The ARIMA model for stock prices have d = 1, and hence, stock is non-stationary during the study period.


#### Fit NNAR Models for stock prices


```{r}
# Initialize an empty data frame to store NNETAR model orders
nnetar_results <- data.frame(
  Stock = character(),
  NNETAR_Order = character(),
  stringsAsFactors = FALSE
)
  
# Fit NNETAR model to the column asset
nnetar_model <- nnetar(asset$AAPL)
    
# Collect NNETAR order (p, P, k)
nnetar_order <- paste("(", nnetar_model$p, ",", nnetar_model$P, ",", nnetar_model$size, ")", sep = "")
    
# Append the results to the data frame
nnetar_results <- data.frame(
      Stock = colnames(asset),
      NNETAR_Order = paste("NNAR", nnetar_order)
    )

print(nnetar_results)

```


Comments on NNAR model: Since P = 0, it is a non-seasonal model.


#### Forecast Accuracy


```{r}
#### Vectors to save forecasts of each model for forecast combinations
Drift.F = c()
ARIMA.F = c()
NNAR.F = c()
LSTM1.F = c()
XGB.F = c()
LSTM2.F = c()

```



#### Price $\sim$ Lag of price (Drift, ARIMA, nnetar, LSTM1, Xgboost, LSTM2, and Random Forest)


```{r}
# split date is '2023-02-05' (Start Date for the Testing Data)
split_date <- as.Date("2023-02-05")  # Replace with your desired date

# Filter data based on the split date
train_data <- asset[row.names(asset) <= split_date, , drop = FALSE]
test_data <- asset[row.names(asset) > split_date, , drop = FALSE]

# Ensure row numbers correspond to dates
row.names(train_data) <- row.names(asset[row.names(asset) <= split_date, , drop = FALSE])
row.names(test_data) <- row.names(asset[row.names(asset) > split_date, , drop = FALSE])

```



#### Drift Model


```{r}
Drift_forecast <- rwf(train_data$AAPL, h = nrow(test_data), drift = TRUE)
Drift.F = Drift_forecast$mean
MAE.Drift = mean(abs(test_data$AAPL - Drift_forecast$mean))
RMSE.Drift = sqrt(mean((test_data$AAPL - Drift_forecast$mean)^2))
MAPE.Drift = mean(abs((test_data$AAPL - Drift_forecast$mean) / test_data$AAPL))*100
```



#### ARIMA Model


```{r}
arima_model <- auto.arima(train_data$AAPL)
arima_model
arima_forecast <- forecast(arima_model, h = nrow(test_data))
ARIMA.F = arima_forecast$mean
MAE.ARIMA = mean(abs(test_data$AAPL - arima_forecast$mean))
RMSE.ARIMA = sqrt(mean((test_data$AAPL - arima_forecast$mean)^2))
MAPE.ARIMA = mean(abs((test_data$AAPL - arima_forecast$mean) / test_data$AAPL))*100

```


#### NNAR Model


```{r}
nnetar_model <- nnetar(train_data$AAPL)
nnetar_model
nnetar_forecast <- forecast(nnetar_model, h = nrow(test_data))
NNAR.F = nnetar_forecast$mean
MAE.NNAR = mean(abs(test_data$AAPL - nnetar_forecast$mean))
RMSE.NNAR = sqrt(mean((test_data$AAPL - nnetar_forecast$mean)^2))
MAPE.NNAR = mean(abs((test_data$AAPL - nnetar_forecast$mean) / test_data$AAPL))*100

```


#### LSTM with Single Hidden Layer


```{r}
### Set path (For Windows computers)

library(reticulate)
use_python('C:/Users/sulal/AppData/Local/Programs/Python/Python38/Python.exe')

library(keras)

```



```{r}
# Initialize vectors for lags and number of neurons
seriesLags <- 1
No.neurons <- 1


# Select the series
series <- train_data$AAPL
  
# Normalize the data (this step is optional but recommended)
min_val <- min(series)
max_val <- max(series)
scaled_series <- (series - min_val) / (max_val - min_val)
  
# Set up parameters for the current series
lookback <- seriesLags  # Number of previous time steps used to predict the next one
neurons <- No.neurons  # Number of neurons in the LSTM layer
train_size <- length(scaled_series)
forecast_length <- nrow(test_data)  # Same test data length for all series
  
# Function to create input-output pairs
create_dataset <- function(data, lookback) {
  X <- list()
  y <- list()
  for (j in seq(lookback, length(data) - 1)) {
    X[[j - lookback + 1]] <- data[(j - lookback + 1):j]
    y[[j - lookback + 1]] <- data[j + 1]
  }
  return(list(X = array(unlist(X), dim = c(length(X), lookback, 1)),
                y = unlist(y)))
}
  
# Create training data
dataset <- create_dataset(scaled_series, lookback)
X_train <- dataset$X
y_train <- dataset$y
  
# Define the LSTM model for the current series
LSTM.Model <- keras_model_sequential() %>%
  layer_lstm(units = neurons, input_shape = c(lookback, 1)) %>%
  layer_dense(units = 1)
  
# Compile the LSTM model
LSTM.Model %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)
  
# Train the LSTM model
LSTM.Model %>% fit(
  X_train, y_train,
  epochs = 20,
  batch_size = 32, 
  verbose = 0
)
  
# Forecast the next Test.Length steps using the LSTM model
lstm_predictions <- numeric(forecast_length)
input_seq_lstm <- scaled_series[(train_size - lookback + 1):train_size]
  
for (j in 1:forecast_length) {
  pred_input_lstm <- array(input_seq_lstm, dim = c(1, lookback, 1))
  next_val_lstm <- LSTM.Model %>% predict(pred_input_lstm)
  lstm_predictions[j] <- next_val_lstm
    
  # Update the input sequence
  input_seq_lstm <- c(input_seq_lstm[-1], next_val_lstm)
}
  
# Reverse scaling to original values for LSTM predictions
fore.LSTM.Price <- lstm_predictions * (max_val - min_val) + min_val
LSTM1.F = fore.LSTM.Price
  
# Calculate accuracy metrics
MAE.LSTM1 <- mean(abs(test_data$AAPL - fore.LSTM.Price))
RMSE.LSTM1 <- sqrt(mean((test_data$AAPL - fore.LSTM.Price)^2))
MAPE.LSTM1 <- mean(abs(100 * (test_data$AAPL - fore.LSTM.Price) / test_data$AAPL))
  
```


#### Xgboost

```{r}
library(xgboost)

```


```{r}
# Select the series
series <- train_data$AAPL
  
# Normalize the data
min_val <- min(series)
max_val <- max(series)
scaled_series <- (series - min_val) / (max_val - min_val)
  
# Set up parameters for the current series
lookback <- seriesLags  # Number of previous time steps used to predict the next one
train_size <- length(scaled_series)
forecast_length <- nrow(test_data)  # Same test data length for all series
  
# Create input-output pairs for training
create_dataset <- function(data, lookback) {
    X <- list()
    y <- list()
    for (j in seq(lookback, length(data) - 1)) {
      X[[j - lookback + 1]] <- data[(j - lookback + 1):j]
      y[[j - lookback + 1]] <- data[j + 1]
    }
    return(list(X = do.call(rbind, X), y = unlist(y)))
  }
  
# Create training data
dataset <- create_dataset(scaled_series, lookback)
X_train <- dataset$X
y_train <- dataset$y
  
# Train XGBoost model
params <- list(
  booster = "gbtree",
  objective = "reg:squarederror", # Regression problem
  eta = 0.1,                      # Learning rate
  max_depth = 6,                  # Maximum depth of trees
  subsample = 0.8,                # Subsample ratio of training data
  colsample_bytree = 0.8          # Subsample ratio of columns
)
  
xgb_model <- xgboost(
  data = X_train,
  label = y_train,
  params = params,
  nrounds = 100,
  verbose = 0
)
  
# Forecast the next forecast_length steps
xgb_predictions <- numeric(forecast_length)
input_seq_xgb <- scaled_series[(train_size - lookback + 1):train_size]
  
for (j in 1:forecast_length) {
  pred_input_xgb <- matrix(input_seq_xgb, nrow = 1)
  next_val_xgb <- predict(xgb_model, pred_input_xgb)
  xgb_predictions[j] <- next_val_xgb
    
  # Update the input sequence
  input_seq_xgb <- c(input_seq_xgb[-1], next_val_xgb)
}
  
# Reverse scaling to original values for XGBoost predictions
fore.XGB.Price <- xgb_predictions * (max_val - min_val) + min_val
XGB.F = fore.XGB.Price
  
# Calculate accuracy metrics
MAE.XGB <- mean(abs(test_data$AAPL - fore.XGB.Price))
RMSE.XGB <- sqrt(mean((test_data$AAPL - fore.XGB.Price)^2))
MAPE.XGB <- mean(abs(100 * (test_data$AAPL - fore.XGB.Price) / test_data$AAPL))
  
```


#### LSTM with Two Hidden Layers


```{r}
# Select the series
series <- train_data$AAPL
  
# Normalize the data (this step is optional but recommended)
min_val <- min(series)
max_val <- max(series)
scaled_series <- (series - min_val) / (max_val - min_val)
  
# Set up parameters for the current series
lookback <- seriesLags  # Number of previous time steps used to predict the next one
neurons <- No.neurons  # Number of neurons in the LSTM layer
train_size <- length(scaled_series)
forecast_length <- nrow(test_data)  # Same test data length for all series
  
# Function to create input-output pairs
create_dataset <- function(data, lookback) {
  X <- list()
  y <- list()
  for (j in seq(lookback, length(data) - 1)) {
    X[[j - lookback + 1]] <- data[(j - lookback + 1):j]
    y[[j - lookback + 1]] <- data[j + 1]
  }
  return(list(X = array(unlist(X), dim = c(length(X), lookback, 1)),
                y = unlist(y)))
}
  
# Create training data
dataset <- create_dataset(scaled_series, lookback)
X_train <- dataset$X
y_train <- dataset$y
  
# Define the LSTM model with two hidden layers
LSTM.Model <- keras_model_sequential() %>%
  layer_lstm(units = neurons, input_shape = c(lookback, 1), return_sequences = TRUE) %>%
    layer_lstm(units = neurons, return_sequences = FALSE) %>%
    layer_dense(units = 1)
  
# Compile the LSTM model
LSTM.Model %>% compile(
  optimizer = 'adam',
  loss = 'mse'
)
  
# Train the LSTM model
LSTM.Model %>% fit(
  X_train, y_train, 
  epochs = 20, 
  batch_size = 32, 
  verbose = 0
)
  
# Forecast the next Test.Length steps using the LSTM model
lstm_predictions <- numeric(forecast_length)
input_seq_lstm <- scaled_series[(train_size - lookback + 1):train_size]
  
for (j in 1:forecast_length) {
  pred_input_lstm <- array(input_seq_lstm, dim = c(1, lookback, 1))
  next_val_lstm <- LSTM.Model %>% predict(pred_input_lstm)
  lstm_predictions[j] <- next_val_lstm
    
  # Update the input sequence
  input_seq_lstm <- c(input_seq_lstm[-1], next_val_lstm)
}
  
# Reverse scaling to original values for LSTM predictions
fore.LSTM.Price <- lstm_predictions * (max_val - min_val) + min_val
LSTM2.F = fore.LSTM.Price
  
# Calculate accuracy metrics
MAE.LSTM2 <- mean(abs(test_data$AAPL - fore.LSTM.Price))
RMSE.LSTM2 <- sqrt(mean((test_data$AAPL - fore.LSTM.Price)^2))
MAPE.LSTM2 <- mean(abs(100 * (test_data$AAPL - fore.LSTM.Price) / test_data$AAPL))
  
```



#### Random Forest

```{r}
library(randomForest)

```



```{r}
# Select the series
series <- train_data$AAPL

# Normalize the data (optional but recommended)
min_val <- min(series)
max_val <- max(series)
scaled_series <- (series - min_val) / (max_val - min_val)

# Create lagged features
create_lagged_features <- function(data, lags) {
  lagged_data <- embed(data, lags + 1)
  X <- lagged_data[, -1, drop = FALSE]
  y <- lagged_data[, 1]
  return(list(X = X, y = y))
}

# Create training data
train_lagged <- create_lagged_features(scaled_series, seriesLags)
X_train <- train_lagged$X
y_train <- train_lagged$y

# Train Random Forest model
rf_model <- randomForest(X_train, y_train, ntree = 500)

# Forecast for the testing period
forecast_length <- nrow(test_data)
rf_predictions <- numeric(forecast_length)
input_seq_rf <- scaled_series[(length(scaled_series) - seriesLags + 1):length(scaled_series)]

for (i in 1:forecast_length) {
  pred_input_rf <- matrix(input_seq_rf, nrow = 1)
  next_val_rf <- predict(rf_model, pred_input_rf)
  rf_predictions[i] <- next_val_rf
  
  # Update the input sequence
  input_seq_rf <- c(input_seq_rf[-1], next_val_rf)
}

# Reverse scaling to original values for RF predictions
fore.RF.Price <- rf_predictions * (max_val - min_val) + min_val
RF.F <- fore.RF.Price

# Calculate accuracy metrics
MAE.RF <- mean(abs(test_data$AAPL - fore.RF.Price))
RMSE.RF <- sqrt(mean((test_data$AAPL - fore.RF.Price)^2))
MAPE.RF <- mean(abs(100 * (test_data$AAPL - fore.RF.Price) / test_data$AAPL))

```




#### Forecast Summary


```{r}
MAE <- c(MAE.Drift, MAE.ARIMA, MAE.NNAR, MAE.LSTM1, MAE.XGB, MAE.LSTM2, MAE.RF)
RMSE <- c(RMSE.Drift, RMSE.ARIMA, RMSE.NNAR, RMSE.LSTM1, RMSE.XGB, RMSE.LSTM2, RMSE.RF)
MAPE <- c(MAPE.Drift, MAPE.ARIMA, MAPE.NNAR, MAPE.LSTM1, MAPE.XGB, MAPE.LSTM2, MAPE.RF)
Model <- c('Drift', 'ARIMA', 'NNAR', 'LSTM1', 'XGB', 'LSTM2', 'RF')
Forecast.Accuracy.Ind <- data.frame(Model, MAE, RMSE, MAPE)
xtable(Forecast.Accuracy.Ind, digits = 3)

write_xlsx(Forecast.Accuracy.Ind,"Price.Forecast.Accuracy.Ind_AAPL.xlsx")
  
```



#### Plot Forecasts of 7 Models


```{r}
# Load required libraries
library(ggplot2)
library(reshape2)

```


```{r}
# Create a data frame for plotting
data <- data.frame(
  Date = seq_along(test_data[, 1]),  # Replace with actual dates if available
  Price = as.numeric(test_data$AAPL),
  Drift = as.numeric(Drift.F),
  ARIMA = as.numeric(ARIMA.F),
  NNAR = as.numeric(NNAR.F),
  LSTM1 = as.numeric(LSTM1.F),
  XGB = as.numeric(XGB.F),
  LSTM2 = as.numeric(LSTM2.F),
  RF = as.numeric(RF.F)
)

# Reshape data for ggplot
plot_data <- reshape2::melt(data, id.vars = "Date", variable.name = "Model", value.name = "Value")

# Create individual plot
p <- ggplot(plot_data, aes(x = Date, y = Value, color = Model)) +
  geom_line(size = 1) +
  scale_color_manual(
    values = c(
      "Price" = "black", 
      "Drift" = "pink",
      "ARIMA" = "brown",  # Match the updated ARIMA color
      "NNAR" = "blue", 
      "LSTM1" = "green", 
      "XGB" = "purple", 
      "LSTM2" = "orange", 
      "RF" = "yellow"
    )
  ) +
  labs(
    title = ticker, 
    x = "Date", 
    y = "Adjusted Closing Price (USD)"  # Updated y-axis label for consistency
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank()  # Remove "Model" from legend title
  )

# Display the plot in the plotting window
print(p)

```






#### Plot Forecasts of 7 Models with training and testing data

```{r}
# Define the training and testing periods
train_length <- length(train_data$AAPL)
test_length <- length(test_data$AAPL)
cutoff <- train_length  # The point where training ends and testing begins

# Create a continuous sequence of dates for both training and testing periods
dates <- seq.Date(from = as.Date("2020-03-11"), by = "day", length.out = train_length + test_length)

# Create a data frame for observed data (training and testing)
data <- data.frame(
  Date = dates,
  Value = c(as.numeric(train_data$AAPL), as.numeric(test_data$AAPL)),
  Type = "Price"  # Unified label for both training and testing periods
)

# Add forecast data for the testing period
forecast_data <- data.frame(
  Date = dates[(cutoff + 1):(cutoff + test_length)],  # Forecast dates match testing period
  Drift = as.numeric(Drift.F),
  ARIMA = as.numeric(ARIMA.F),
  NNAR = as.numeric(NNAR.F),
  LSTM1 = as.numeric(LSTM1.F),
  XGB = as.numeric(XGB.F),
  LSTM2 = as.numeric(LSTM2.F),
  RF = as.numeric(RF.F)
)

# Reshape forecast data for ggplot
forecast_data_melt <- reshape2::melt(forecast_data, id.vars = "Date", variable.name = "Model", value.name = "Value")

# Combine observed data and forecasts
observed_data <- data.frame(Date = data$Date, Value = data$Value, Model = data$Type)
combined_data <- rbind(observed_data, forecast_data_melt)

# Create the plot
p <- ggplot(combined_data, aes(x = Date, y = Value, color = Model)) +
  geom_line(size = 1) +
  geom_vline(xintercept = as.numeric(dates[cutoff]), linetype = "dashed", color = "red", size = 1) +  # Enhanced vertical line
  annotate("text", x = dates[cutoff] - 60, y = max(combined_data$Value) * 1.05, 
           label = "Training", color = "red", angle = 0, hjust = 0.5) +  # Horizontal Training Period
  annotate("text", x = dates[cutoff] + 50, y = max(combined_data$Value) * 1.05, 
           label = "Testing", color = "red", angle = 0, hjust = 0.5) +  # Horizontal Testing Period
  scale_color_manual(
    values = c(
      "Price" = "black", 
      "Drift" = "pink",
      "ARIMA" = "brown",  # Changed ARIMA color
      "NNAR" = "blue", 
      "LSTM1" = "green", 
      "XGB" = "purple", 
      "LSTM2" = "orange", 
      "RF" = "yellow"
    )
  ) +
  labs(title = ticker, x = "Date", y = "Adjusted Closing Price (USD)") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank()  # Remove "Model" from legend title
  )

# Display the plot in the plotting window
print(p)

```


```{r}
# Define the training and testing periods
train_length <- length(train_data$AAPL)
test_length <- length(test_data$AAPL)
cutoff <- train_length  # The point where training ends and testing begins

# Create a continuous sequence of dates for both training and testing periods
dates <- seq.Date(from = as.Date("2020-03-11"), by = "day", length.out = train_length + test_length)

# Create a data frame for observed data (training and testing)
data <- data.frame(
  Date = dates,
  Value = c(as.numeric(train_data$AAPL), as.numeric(test_data$AAPL)),
  Type = "Price"  # Unified label for both training and testing periods
)

# Add forecast data for the testing period
forecast_data <- data.frame(
  Date = dates[(cutoff + 1):(cutoff + test_length)],  # Forecast dates match testing period
  Drift = as.numeric(Drift.F),
  ARIMA = as.numeric(ARIMA.F),
  NNAR = as.numeric(NNAR.F),
  LSTM1 = as.numeric(LSTM1.F),
  XGB = as.numeric(XGB.F),
  LSTM2 = as.numeric(LSTM2.F),
  RF = as.numeric(RF.F)
)

# Reshape forecast data for ggplot
forecast_data_melt <- reshape2::melt(forecast_data, id.vars = "Date", variable.name = "Model", value.name = "Value")

# Combine observed data and forecasts
observed_data <- data.frame(Date = data$Date, Value = data$Value, Model = data$Type)
combined_data <- rbind(observed_data, forecast_data_melt)

# Create the plot
p <- ggplot(combined_data, aes(x = Date, y = Value, color = Model)) +
  geom_line(size = 1) +
  geom_vline(xintercept = as.numeric(dates[cutoff]), linetype = "dashed", color = "red", size = 1) +  # Enhanced vertical line
  annotate("text", x = dates[cutoff] - 60, y = max(combined_data$Value) * 1.05, 
           label = "Training", color = "black", angle = 0, hjust = 0.5) +  # Horizontal Training Label
  annotate("segment", 
           x = dates[cutoff] - 80, xend = dates[cutoff] - 40,  # Longer arrow under "Training"
           y = max(combined_data$Value) * 1.00, yend = max(combined_data$Value) * 1.00, 
           color = "red", arrow = arrow(length = unit(0.2, "cm"), ends = "first")) +
  annotate("text", x = dates[cutoff] + 50, y = max(combined_data$Value) * 1.05, 
           label = "Testing", color = "black", angle = 0, hjust = 0.5) +  # Horizontal Testing Label
  annotate("segment", 
           x = dates[cutoff] + 20, xend = dates[cutoff] + 60,  # Longer arrow under "Testing"
           y = max(combined_data$Value) * 1.00, yend = max(combined_data$Value) * 1.00, 
           color = "red", arrow = arrow(length = unit(0.2, "cm"), ends = "last")) +
  scale_color_manual(
    values = c(
      "Price" = "black", 
      "Drift" = "pink",
      "ARIMA" = "brown",  # Changed ARIMA color
      "NNAR" = "blue", 
      "LSTM1" = "green", 
      "XGB" = "purple", 
      "LSTM2" = "orange", 
      "RF" = "yellow"
    )
  ) +
  labs(title = ticker, x = "Date", y = "Adjusted Closing Price (USD)") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.title = element_blank()  # Remove "Model" from legend title
  )

# Display the plot in the plotting window
print(p)


```


#### Forecast Combinations


```{r}
#### Two Models

### Function to calculate MAE, RMSE, and MAPE for two models
calculate_forecast_errors <- function(name, actual, forecast1, forecast2) {
  # Combine the two forecast series into one by averaging
  combined_forecast <- (forecast1 + forecast2) / 2
  
  # Calculate forecast errors
  MAE <- mean(abs(actual - combined_forecast), na.rm = TRUE)
  RMSE <- sqrt(mean((actual - combined_forecast)^2, na.rm = TRUE))
  MAPE <- mean(abs(100 * (actual - combined_forecast) / actual), na.rm = TRUE)
  
  # Return the results as a data frame
  result <- data.frame(
    Model = name,
    MAE = MAE,
    RMSE = RMSE,
    MAPE = MAPE
  )
  
  return(result)
}


#### Drift and ARIMA
Drift.ARIMA <- calculate_forecast_errors('Drift.ARIMA', test_data$AAPL, Drift.F, ARIMA.F)

#### Drift and NNAR
Drift.NNAR <- calculate_forecast_errors('Drift.NNAR', test_data$AAPL, Drift.F, NNAR.F)

#### Drift and LSTM1
Drift.LSTM1 <- calculate_forecast_errors('Drift.LSTM1', test_data$AAPL, Drift.F, LSTM1.F)

#### Drift and XGB
Drift.XGB <- calculate_forecast_errors('Drift.XGB', test_data$AAPL, Drift.F, XGB.F)

#### Drift and LSTM2
Drift.LSTM2 <- calculate_forecast_errors('Drift.LSTM2', test_data$AAPL, Drift.F, LSTM2.F)

#### Drift and RF
Drift.RF <- calculate_forecast_errors('Drift.RF', test_data$AAPL, Drift.F, RF.F)

#### ARIMA and NNAR
ARIMA.NNAR <- calculate_forecast_errors('ARIMA.NNAR', test_data$AAPL, ARIMA.F, NNAR.F)

#### ARIMA and LSTM
ARIMA.LSTM1 <- calculate_forecast_errors('ARIMA.LSTM1', test_data$AAPL, ARIMA.F, LSTM1.F)

#### ARIMA and XGB
ARIMA.XGB <- calculate_forecast_errors('ARIMA.XGB', test_data$AAPL, ARIMA.F, XGB.F)

#### ARIMA and LSTM2
ARIMA.LSTM2 <- calculate_forecast_errors('ARIMA.LSTM2', test_data$AAPL, ARIMA.F, LSTM2.F)

#### ARIMA and RF
ARIMA.RF <- calculate_forecast_errors('ARIMA.RF', test_data$AAPL, ARIMA.F, RF.F)

#### NNAR and LSTM1
NNAR.LSTM1 <- calculate_forecast_errors('NNAR.LSTM1', test_data$AAPL, NNAR.F, LSTM1.F)

#### NNAR and XGB
NNAR.XGB <- calculate_forecast_errors('NNAR.XGB', test_data$AAPL, NNAR.F, XGB.F)

#### NNAR and LSTM2
NNAR.LSTM2 <- calculate_forecast_errors('NNAR.LSTM2', test_data$AAPL, NNAR.F, LSTM2.F)

#### NNAR and RF
NNAR.RF <- calculate_forecast_errors('NNAR.RF', test_data$AAPL, NNAR.F, RF.F)

#### LSTM1 and XGB
LSTM1.XGB <- calculate_forecast_errors('LSTM1.XGB', test_data$AAPL, LSTM1.F, XGB.F)

#### LSTM1 and LSTM2
LSTM1.LSTM2 <- calculate_forecast_errors('LSTM1.LSTM2', test_data$AAPL, LSTM1.F, LSTM2.F)

#### LSTM1 and RF
LSTM1.RF <- calculate_forecast_errors('LSTM1.RF', test_data$AAPL, LSTM1.F, RF.F)

#### XGB and LSTM2
XGB.LSTM2 <- calculate_forecast_errors('XGB.LSTM2', test_data$AAPL, XGB.F, LSTM2.F)

#### XGB and RF
XGB.RF <- calculate_forecast_errors('XGB.RF', test_data$AAPL, XGB.F, RF.F)

#### Summary - Two Models
Forecast.Accuracy.Two <- rbind.data.frame(Drift.ARIMA, Drift.NNAR, Drift.LSTM1, Drift.XGB, Drift.LSTM2, Drift.RF, ARIMA.NNAR, ARIMA.LSTM1, ARIMA.XGB, ARIMA.LSTM2, ARIMA.RF, NNAR.LSTM1, NNAR.XGB, NNAR.LSTM2, NNAR.RF, LSTM1.XGB, LSTM1.LSTM2, LSTM1.RF, XGB.LSTM2, XGB.RF)
xtable(Forecast.Accuracy.Two, digits = 3)

write_xlsx(Forecast.Accuracy.Two,"Price.Forecast.Accuracy.Equal.Weight_AAPL.xlsx")

```


#### Summary of all models (individual and two)

```{r}
Summary <- rbind.data.frame(Forecast.Accuracy.Ind, Forecast.Accuracy.Two)

xtable(Summary, digits = 3)

```


#### Optimal Weights for the Combinations (two models)


```{r}
#### Function to find Optimal weights
find_optimal_weight <- function(name1, name2, Obs, Fore1, Fore2) {
  # Define the sequence of weights to evaluate
  Weight <- seq(0.01, 0.99, 0.01)
  SSE_Weight <- rep(0, length(Weight))
  T <- length(Obs)
  
  # Loop over each weight and calculate the SSE
  for (i in 1:length(Weight)) {
    Fore <- Weight[i] * Fore1 + (1 - Weight[i]) * Fore2
    SSE <- 0
    
    for (j in 1:T) {
      SSE <- SSE + (Obs[j] - Fore[j])^2
    }
    
    SSE_Weight[i] <- SSE
  }
  
  # Find the optimal weight
  Weight.Opt <- Weight[which.min(SSE_Weight)]
  
  # Create the return data frame
  result <- data.frame(
    Model = paste(name1, name2, sep = "."),
    Weight.Opt = Weight.Opt,
    Complement.Weight = 1 - Weight.Opt
  )
  
  colnames(result)[2:3] <- c('Model 1 Weight', 'Model 2 Weight')
  
  # Create a data frame for plotting
  plot_data <- data.frame(Weight = Weight, SSE_Weight = SSE_Weight)
  
  # Plot SSE_Weight against Weight
  p <- ggplot(plot_data, aes(x = Weight, y = SSE_Weight)) +
    geom_line(color = "blue", size = 1) +
    labs(
      title = paste(name1, name2, sep = "."),
      x = "Weight",
      y = "Forecast Error Sum of Squares (FESS)"
    ) +
    theme_minimal()
  
  print(p)
  
  return(result)
}


#### Drift and ARIMA
optimal_weights.Drift.ARIMA <- find_optimal_weight("Drift", "ARIMA", test_data$AAPL, Drift.F, ARIMA.F)

#### Drift and NNAR
optimal_weights.Drift.NNAR <- find_optimal_weight("Drift", "NNAR", test_data$AAPL, Drift.F, NNAR.F)

#### Drift and LSTM1
optimal_weights.Drift.LSTM1 <- find_optimal_weight("Drift", "LSTM1", test_data$AAPL, Drift.F, LSTM1.F)

#### Drift and XGB
optimal_weights.Drift.XGB <- find_optimal_weight("Drift", "XGB", test_data$AAPL, Drift.F, XGB.F)

#### Drift and LSTM2
optimal_weights.Drift.LSTM2 <- find_optimal_weight("Drift", "LSTM2", test_data$AAPL, Drift.F, LSTM2.F)

#### Drift and RF
optimal_weights.Drift.RF <- find_optimal_weight("Drift", "RF", test_data$AAPL, Drift.F, RF.F)

#### ARIMA and NNAR
optimal_weights.ARIMA.NNAR <- find_optimal_weight("ARIMA", "NNAR", test_data$AAPL, ARIMA.F, NNAR.F)

#### ARIMA and LSTM
optimal_weights.ARIMA.LSTM1 <- find_optimal_weight("ARIMA", "LSTM1", test_data$AAPL, ARIMA.F, LSTM1.F)

#### ARIMA and XGB
optimal_weights.ARIMA.XGB <- find_optimal_weight("ARIMA", "XGB", test_data$AAPL, ARIMA.F, XGB.F)

#### ARIMA and LSTM2
optimal_weights.ARIMA.LSTM2 <- find_optimal_weight("ARIMA", "LSTM2", test_data$AAPL, ARIMA.F, LSTM2.F)

#### ARIMA and RF
optimal_weights.ARIMA.RF <- find_optimal_weight("ARIMA", "RF", test_data$AAPL, ARIMA.F, RF.F)

#### NNAR and LSTM1
optimal_weights.NNAR.LSTM1 <- find_optimal_weight("NNAR", "LSTM1", test_data$AAPL, NNAR.F, LSTM1.F)

#### NNAR and XGB
optimal_weights.NNAR.XGB <- find_optimal_weight("NNAR", "XGB", test_data$AAPL, NNAR.F, XGB.F)

#### NNAR and LSTM2
optimal_weights.NNAR.LSTM2 <- find_optimal_weight("NNAR", "LSTM2", test_data$AAPL, NNAR.F, LSTM2.F)

#### NNAR and RF
optimal_weights.NNAR.RF <- find_optimal_weight("NNAR", "RF", test_data$AAPL, NNAR.F, RF.F)

#### LSTM1 and XGB
optimal_weights.LSTM1.XGB <- find_optimal_weight("LSTM1", "XGB", test_data$AAPL, LSTM1.F, XGB.F)

#### LSTM1 and LSTM2
optimal_weights.LSTM1.LSTM2 <- find_optimal_weight("LSTM1", "LSTM2", test_data$AAPL, LSTM1.F, LSTM2.F)

#### LSTM1 and RF
optimal_weights.LSTM1.RF <- find_optimal_weight("LSTM1", "RF", test_data$AAPL, LSTM1.F, RF.F)

#### XGB and LSTM2
optimal_weights.XGB.LSTM2 <- find_optimal_weight("XGB", "LSTM2", test_data$AAPL, XGB.F, LSTM2.F)

#### XGB and RF
optimal_weights.XGB.RF <- find_optimal_weight("XGB", "RF", test_data$AAPL, XGB.F, RF.F)

#### Summary - Two Models
optimal_weights.Two <- rbind.data.frame(optimal_weights.Drift.ARIMA, optimal_weights.Drift.NNAR, optimal_weights.Drift.LSTM1, optimal_weights.Drift.XGB, optimal_weights.Drift.LSTM2, optimal_weights.Drift.RF, optimal_weights.ARIMA.NNAR, optimal_weights.ARIMA.LSTM1, optimal_weights.ARIMA.XGB, optimal_weights.ARIMA.LSTM2, optimal_weights.ARIMA.RF, optimal_weights.NNAR.LSTM1, optimal_weights.NNAR.XGB, optimal_weights.NNAR.LSTM2, optimal_weights.NNAR.RF, optimal_weights.LSTM1.XGB, optimal_weights.LSTM1.LSTM2, optimal_weights.LSTM1.RF, optimal_weights.XGB.LSTM2, optimal_weights.XGB.RF)
xtable(optimal_weights.Two, digits = 2)

xtable(optimal_weights.Two)

```



#### Forecast Accuracies with Optimal Weights

```{r}
### Function to calculate MAE, RMSE, and MAPE for two models with custom weights
calculate_forecast_errors.Opt <- function(name, actual, forecast1, forecast2, X) {

    # Combine the two forecast series using weights
  combined_forecast <- X * forecast1 + (1 - X) * forecast2
  
  # Calculate forecast errors
  MAE <- mean(abs(actual - combined_forecast), na.rm = TRUE)
  RMSE <- sqrt(mean((actual - combined_forecast)^2, na.rm = TRUE))
  MAPE <- mean(abs(100 * (actual - combined_forecast) / actual), na.rm = TRUE)
  
  # Return the results as a data frame
  result <- data.frame(
    Model = name,
    MAE = MAE,
    RMSE = RMSE,
    MAPE = MAPE
  )
  
  return(result)
}


#### Drift and ARIMA
Drift.ARIMA <- calculate_forecast_errors.Opt('Drift.ARIMA', test_data$AAPL, Drift.F, ARIMA.F, optimal_weights.Two[1,2])

#### Drift and NNAR
Drift.NNAR <- calculate_forecast_errors.Opt('Drift.NNAR', test_data$AAPL, Drift.F, NNAR.F, optimal_weights.Two[2,2])

#### Drift and LSTM1
Drift.LSTM1 <- calculate_forecast_errors.Opt('Drift.LSTM1', test_data$AAPL, Drift.F, LSTM1.F, optimal_weights.Two[3,2])

#### Drift and XGB
Drift.XGB <- calculate_forecast_errors.Opt('Drift.XGB', test_data$AAPL, Drift.F, XGB.F, optimal_weights.Two[4,2])

#### Drift and LSTM2
Drift.LSTM2 <- calculate_forecast_errors.Opt('Drift.LSTM2', test_data$AAPL, Drift.F, LSTM2.F, optimal_weights.Two[5,2])

#### Drift and RF
Drift.RF <- calculate_forecast_errors.Opt('Drift.RF', test_data$AAPL, Drift.F, RF.F, optimal_weights.Two[6,2])

#### ARIMA and NNAR
ARIMA.NNAR <- calculate_forecast_errors.Opt('ARIMA.NNAR', test_data$AAPL, ARIMA.F, NNAR.F, optimal_weights.Two[7,2])

#### ARIMA and LSTM
ARIMA.LSTM1 <- calculate_forecast_errors.Opt('ARIMA.LSTM1', test_data$AAPL, ARIMA.F, LSTM1.F, optimal_weights.Two[8,2])

#### ARIMA and XGB
ARIMA.XGB <- calculate_forecast_errors.Opt('ARIMA.XGB', test_data$AAPL, ARIMA.F, XGB.F, optimal_weights.Two[9,2])

#### ARIMA and LSTM2
ARIMA.LSTM2 <- calculate_forecast_errors.Opt('ARIMA.LSTM2', test_data$AAPL, ARIMA.F, LSTM2.F, optimal_weights.Two[10,2])

#### ARIMA and RF
ARIMA.RF <- calculate_forecast_errors.Opt('ARIMA.RF', test_data$AAPL, ARIMA.F, RF.F, optimal_weights.Two[11,2])

#### NNAR and LSTM1
NNAR.LSTM1 <- calculate_forecast_errors.Opt('NNAR.LSTM1', test_data$AAPL, NNAR.F, LSTM1.F, optimal_weights.Two[12,2])

#### NNAR and XGB
NNAR.XGB <- calculate_forecast_errors.Opt('NNAR.XGB', test_data$AAPL, NNAR.F, XGB.F, optimal_weights.Two[13,2])

#### NNAR and LSTM2
NNAR.LSTM2 <- calculate_forecast_errors.Opt('NNAR.LSTM2', test_data$AAPL, NNAR.F, LSTM2.F, optimal_weights.Two[14,2])

#### NNAR and RF
NNAR.RF <- calculate_forecast_errors.Opt('NNAR.RF', test_data$AAPL, NNAR.F, RF.F, optimal_weights.Two[15,2])

#### LSTM1 and XGB
LSTM1.XGB <- calculate_forecast_errors.Opt('LSTM1.XGB', test_data$AAPL, LSTM1.F, XGB.F, optimal_weights.Two[16,2])

#### LSTM1 and LSTM2
LSTM1.LSTM2 <- calculate_forecast_errors.Opt('LSTM1.LSTM2', test_data$AAPL, LSTM1.F, LSTM2.F, optimal_weights.Two[17,2])

#### LSTM1 and RF
LSTM1.RF <- calculate_forecast_errors.Opt('LSTM1.RF', test_data$AAPL, LSTM1.F, RF.F, optimal_weights.Two[18,2])

#### XGB and LSTM2
XGB.LSTM2 <- calculate_forecast_errors.Opt('XGB.LSTM2', test_data$AAPL, XGB.F, LSTM2.F, optimal_weights.Two[19,2])

#### XGB and RF
XGB.RF <- calculate_forecast_errors.Opt('XGB.RF', test_data$AAPL, XGB.F, RF.F, optimal_weights.Two[20,2])

#### Summary - Two Models
Forecast.Accuracy.Two.Opt <- rbind.data.frame(Drift.ARIMA, Drift.NNAR, Drift.LSTM1, Drift.XGB, Drift.LSTM2, Drift.RF, ARIMA.NNAR, ARIMA.LSTM1, ARIMA.XGB, ARIMA.LSTM2, ARIMA.RF, NNAR.LSTM1, NNAR.XGB, NNAR.LSTM2, NNAR.RF, LSTM1.XGB, LSTM1.LSTM2, LSTM1.RF, XGB.LSTM2, XGB.RF)
xtable(Forecast.Accuracy.Two.Opt, digits = 3)

write_xlsx(Forecast.Accuracy.Two.Opt,"Price.Forecast.Accuracy.Opt.Weight_AAPL.xlsx")

```



#### Summary of models (individual and optimal weights)

```{r}
Summary <- rbind.data.frame(Forecast.Accuracy.Ind, Forecast.Accuracy.Two.Opt)

xtable(Summary, digits = 3)

```


```{r}
# Find the row with the minimum value for each metric
min_mae_row <- Summary[which.min(Summary$MAE), ]
min_rmse_row <- Summary[which.min(Summary$RMSE), ]
min_mape_row <- Summary[which.min(Summary$MAPE), ]

# Display the rows
cat("Row with minimum MAE:\n")
print(min_mae_row)

cat("\nRow with minimum RMSE:\n")
print(min_rmse_row)

cat("\nRow with minimum MAPE:\n")
print(min_mape_row)

```



#### Forecast Error Sum of Squares (FESS) Plot for the Best Model (RMSE)

```{r}
find_optimal_weight_with_features <- function(name1, name2, Obs, Fore1, Fore2) {
  # Define the sequence of weights to evaluate
  Weight <- seq(0.01, 0.99, 0.01)
  SSE_Weight <- rep(0, length(Weight))
  T <- length(Obs)
  
  # Loop over each weight and calculate the SSE
  for (i in 1:length(Weight)) {
    Fore <- Weight[i] * Fore1 + (1 - Weight[i]) * Fore2
    SSE <- sum((Obs - Fore)^2)
    SSE_Weight[i] <- SSE
  }
  
  # Find the optimal weight
  Weight.Opt <- Weight[which.min(SSE_Weight)]
  SSE.Min <- min(SSE_Weight)
  
  # Create the return data frame
  result <- data.frame(
    Model = paste(name1, name2, sep = "."),
    Weight.Opt = Weight.Opt,
    Complement.Weight = 1 - Weight.Opt
  )
  
  colnames(result)[2:3] <- c('Model 1 Weight', 'Model 2 Weight')
  
  # Create a data frame for plotting
  plot_data <- data.frame(Weight = Weight, SSE_Weight = SSE_Weight)
  
  # Plot SSE_Weight against Weight with additional features
  p <- ggplot(plot_data, aes(x = Weight, y = SSE_Weight)) +
    geom_line(color = "blue", size = 1) +
    geom_vline(xintercept = Weight.Opt, color = "red", linetype = "dashed", size = 1) +
    annotate(
      "text", 
      x = Weight.Opt + 0.05, 
      y = SSE.Min + 0.05 * (max(SSE_Weight) - min(SSE_Weight)), 
      label = paste0(round(Weight.Opt, 2)), 
      color = "red", 
      size = 4, 
      hjust = 0.5  # Center text horizontally on the line
    ) +
    scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
    labs(
      title = paste(name1, name2, sep = "."),
      x = "Weight",
      y = "Forecast Error Sum of Squares (FESS)"
    ) +
    theme_minimal()
  
  print(p)
  
  return(result)
}


find_optimal_weight_with_features("Drift", "ARIMA", test_data$AAPL, Drift.F, ARIMA.F)

```



